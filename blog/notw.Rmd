---
title: "Text Mining on The Name of The Wind"
author: "Omar Lopez Rubio"
subtitle: Based on *Text Mining with R*
---


The Name of The Wind, by Patrick Rothfuss, is the first book in the trilogy
*The Kingkiller Chronicle*. Since it came out 13 years ago, it has been
my favourite book.
I recently finished the online book 
[*Text Mining with R*](https://www.tidytextmining.com/), by Julia Silge and and David Robinson, so I wanted to make some analysis on this wonderful text, and try to put some of the new concepts I have on the tidyverse.

First I'm going to do some cleaning on the text. Note that I'm going to filter `stop_words`(from the tidytext package) and also I'm going to remove some custom words that are not really significant for my analysis.
A neat trick from the book, is that you can add a chapter regex in addition to cumsum.
```{r, message = FALSE}
library(tidytext)
library(tidyverse)

notw <- as_tibble(read_delim("C:/Users/omarl/OneDrive/Escritorio/R/notw.txt",
                   delim = "\n", col_names = c("text")))


custom_stop_words <- bind_rows(stop_words,
                               tibble(word = c("id", "im", "youre"),
                                      lexicon = c("custom", "custom", "custom")))

notw_processed <- notw %>%
  mutate(linenumber = row_number(),
         chapter = cumsum(str_detect(text, regex("^chapter", 
                                                 ignore_case = TRUE)))) %>%
  unnest_tokens(word, text) %>%
  mutate(word = str_remove(word, "'")) %>%
  filter(!word %in% custom_stop_words$word) 
```


Now we can see the most common words in the book. `fct_reorder` is a must have in this kind of plots.

```{r}
notw_processed %>%
  count(word, sort = TRUE) %>%
  head(20) %>%
  mutate(word = as_factor(word)) %>%
  mutate(word = fct_reorder(word, n)) %>%
  ggplot(aes(x = word, y = n)) + geom_col() + coord_flip() + 
  labs(x = "", title = "Most common words in The Name of The Wind",
       subtitle = "Stop words and numbers are excluded")
```

I have read this book between 7-9 times. I know exactly why looked, hand and eyes are there. That's because Kvothe, the main character, is in love with Denna, and there are constant references to her.

Let's see for the first 12 chapters the most common words. There is also a really handy function for `geom_col` with `facet_wrap`, called `reorder_within`, so we can see the plot sorted.

```{r}
notw_processed %>%
  filter(chapter < 13) %>%
  count(chapter, word) %>%
  group_by(chapter) %>%
  top_n(10, n) %>%
  slice(1:10) %>%
  ungroup() %>%
  mutate(word = as_factor(word)) %>%
  mutate(word = reorder_within(word, n, chapter)) %>%
  ggplot(aes(x = word, y = n)) + geom_col() + coord_flip() + 
  facet_wrap(~chapter, scale = "free") + scale_x_reordered() 

```

We can divide the chapters between the ones where Kvothe is telling his past history to the Chronicler, and the chapters where Kvothe is talking normally, we would said "present".


The problem doing analysis of a single word is that there are many adjectives and adverbs that can completely modify the words (i.e. "I don't like"). So I'm going to use the `ngrams` value for `token` in `unest_tokens`, so I can get groups of 2 words, and check the most common bigrams.

```{r, message = FALSE}

library(ggraph)
library(igraph)
library(varhandle)

notw_bigrams <- notw %>%
  unnest_tokens(bigram, text, token = "ngrams", n = 2) %>%
  separate(bigram, c("word1", "word2"), sep = " ") %>%
  filter(!word1 %in% custom_stop_words$word,
         !word2 %in% custom_stop_words$word,
         !varhandle::check.numeric(word1),
         !varhandle::check.numeric(word2))

bigrams_count <- notw_bigrams %>%
  count(word1,word2, sort = TRUE)

bigrams_count %>%
  unite(word, word1, word2, sep = " ") %>%
  top_n(20) %>%
  mutate(word = as_factor(word)) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(x = word, y = n)) + geom_col() + coord_flip() + 
  labs(title = "Most common bigrams in The Name of The Wind", x = "",
       subtitle = "Stop words and numbers are excluded")

```

There are some names, like `elxa dal`, but also some cool things like `deep breath`. If you have been reading the book, you know why it's here, so as with `blue fire` and `dark hair`.

So, let's plot the bigrams as a graph

```{r}
library(igraph)
bigram_graph <- bigrams_count %>%
  head(80) %>%
  graph_from_data_frame()

a <- grid::arrow(type = "closed", length = unit(.15, "inches"))

ggraph(bigram_graph, layout = "fr") +
  geom_edge_link(aes(edge_alpha = n), show.legend = FALSE,
                 arrow = a, end_cap = circle(.07, 'inches')) +
  geom_node_point(color = "lightblue", size = 5) +
  geom_node_text(aes(label = name), vjust = 1, hjust = 1) +
  theme_void() + labs(title = "Graph of the most common bigrams in The Name of The Wind",
                      subtitle = "Numbers and stop words are ommited")

```


One of the most cool things I learned that tidytext can do is sentiment analysis in a really simple way.

```{r, message = FALSE}
notw_processed %>%
  inner_join(get_sentiments("afinn")) %>%
  group_by(chapter) %>%
  summarize(sentiment = sum(value)) %>%
  ggplot(aes(x = chapter, y = sentiment)) + geom_point(aes(color = sentiment > 0,
                                                           size = abs(sentiment))) +
  geom_abline(slope = 0, intercept = 0, color = "black") + geom_line(color = "black",
                                                                     alpha = 0.6) + 
  theme(legend.position = "None") + 
  labs(title = "Sentiment progression by Chapter in The Name of The Wind",
       subtitle = "AFINN sentiment dataset used") +
  scale_x_continuous(breaks = seq(0,100,10)) + 
  scale_y_continuous(breaks = seq(-250,100, 30))

```

We have to remember that those are single words, not bigrams. One thing
that we could to get a more accurate analysis is getting the bigrams for each line and check if the first word is a negation. Then multiply by -1 the second term.

So far so good! The book is a drama. Let's check `index = 25`


```{r}
notw_processed %>% 
  inner_join(get_sentiments("afinn")) %>% 
  group_by(index = linenumber %/% 80) %>% 
  filter(index == 25) %>%
  arrange(value)
```

So, in total, we have a sentiment value of...
```{r, message = FALSE}
notw_processed %>%
  inner_join(get_sentiments("afinn")) %>%
  summarize(value = sum(value))
```

Using `widyr`, we can also count the pairs of words appearing in the same
chapter and plot it in a graph.
```{r}
library(widyr)
notw_processed_pairs <- notw_processed %>%
  filter(word != "chapter") %>%
  pairwise_count(word, chapter, sort = TRUE, upper = FALSE) 

set.seed(1234)
notw_processed_pairs %>%
  filter(n >=72) %>%
  graph_from_data_frame() %>%
  ggraph(layout = "fr") +
  geom_edge_link(aes(edge_alpha = n, edge_width = n), edge_colour = "darkred") +
  geom_node_point(size = 5) +
  geom_node_text(aes(label = name), repel = TRUE, 
                 point.padding = unit(0.2, "lines")) +
  theme_void()

```

Tidytext also has a `bind_tf_idf` function, so we can check the most relevant words in a chapter

```{r, message = FALSE}
desc_tf_idf <- notw_processed %>% 
  count(chapter, word, sort = TRUE) %>%
  bind_tf_idf(word, chapter, n) %>%
  group_by(chapter) %>%
  top_n(10, tf_idf) %>%
  slice(1:3) %>%
  ungroup() %>%
  arrange(chapter, desc(tf_idf))
```


Finally, I'm going to do some LDA modeling. Beta is the probability of a
word being generated from that topic.

```{r, message = FALSE}

word_counts <- notw_processed %>%
  count(chapter, word, sort = TRUE)


desc_dtm <- word_counts %>%
  cast_dtm(chapter, word, n)

library(topicmodels)

desc_lda <- LDA(desc_dtm, k = 4, control = list(seed = 1234))
tidy_lda <- tidy(desc_lda)

tidy_lda
top_terms <- tidy_lda %>%
  group_by(topic) %>%
  top_n(10, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)

top_terms %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  group_by(topic, term) %>%    
  arrange(desc(beta)) %>%  
  ungroup() %>%
  ggplot(aes(term, beta, fill = as.factor(topic))) +
  geom_col(show.legend = FALSE) +
  coord_flip() +
  scale_x_reordered() +
  labs(title = "Top 10 terms in each LDA topic",
       x = NULL, y = expression(beta)) +
  facet_wrap(~ topic, ncol = 4, scales = "free")

```

Remember that `k` is an hyperparameter, and in this case I knew that with 4 I could get the best of it. We can see 4 blatantly different topics. The first one is the "present" of Kvothe, the second one is when he's talking about Denna, the third one is when he's studying in the University and the last one is from his childhood. Really impressive.

I can't recommend enough those two books.
Thanks for reading!
